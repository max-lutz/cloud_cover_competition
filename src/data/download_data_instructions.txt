# HOW TO DOWNLOAD THE CLOUD COVER COMPETITION IMAGES

======================================

Welcome to the On Cloud N: Cloud Cover Detection Challenge! 
These instructions will help you access the satellite image files for this competition. 
The imagery and labels are hosted in a set of Azure Blob Storage containers in three regions.

Here are the steps to download the entire training set features and labels to your local machine.

1. Save the `download_data.py` script from the competition data download page:

    https://www.drivendata.org/competitions/83/cloud-cover/data/

2. Install the requirements with the command below. You may want to create and activate a Python (>3.6) 
virtual environment first, for example with conda (https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).

    pip install "cloudpathlib[azure]" loguru tqdm typer

3. To download from the competition container, you'll need a Shared Access Signature (SAS). 
The competition data is stored on three Azure Blob Containers in different regions. 
Each region includes identical data, so choose the region closest to the machine you are downloading the data to. 
If your code is running on the Planetary Computer Hub, use Europe, which is where the cluster is located. 
Save the SAS that you want to use in a file, eg. `sas.txt`. SAS tokens by region:

   - Europe: https://cloudcoverdatawesteurope.blob.core.windows.net/public?se=2022-08-01T12%3A00Z&sp=rl&sv=2018-11-09&sr=c&sig=DrqaBLSI9t1nnx1sekyPaMgsqMiO9%2BBzjU/JwDhfQ64%3D
   - US: https://cloudcoverdatacentralus.blob.core.windows.net/public?se=2022-08-01T12%3A00Z&sp=rl&sv=2018-11-09&sr=c&sig=Bhyvh/jgnWKtcBbZ62nOJKalUByIzDikBenFxLJs7FU%3D
   - Asia: https://cloudcoverdataeastasia.blob.core.windows.net/public?se=2022-08-01T12%3A00Z&sp=rl&sv=2018-11-09&sr=c&sig=nL3TY7pT/tSppIfZ13UeCXvrNE/wT9o0rTXlyJi8aic%3D

4. Run the download script. By default, this will download the entire competition dataset to a directory 
named `data` in your current working directory. You can change the destination with the `--destination-directory` argument.

      `python download_data.py --sas-url <path to sas file>`

  For example, if I am in the US and have saved the relevant SAS token in `sas.txt`:

      `python download_data.py --sas-url sas.txt`

  You can also download a single directory containing a subset of the data with the `--cloud-directory` flag. For example:

      # train features
      `python download_data.py --cloud-directory "az://./train_features" --sas-url sas.txt`

      # train labels
      `python download_data.py --cloud-directory "az://./train_labels" --sas-url sas.txt`

      # single chip
      `python download_data.py --cloud-directory "az://./train_features/akny" --sas-url sas.txt`

## Download script documentation

  ```
  $ python download_data.py --help

  Usage: download_data.py [OPTIONS]

    Downloads the challenge dataset to your local machine.

  Options:
    --sas-url TEXT                  Shared Access Signature URL that allows you
                                    to access the files (starting with
                                    https://...). This can be either the SAS URL
                                    itself or a path to a file containing the
                                    SAS URL, available from the competition
                                    datasets page.  [required]
    --cloud-directory TEXT          Cloudpathlib URI (`az://./<directory>`) for
                                    cloud directory to be downloaded.  [default:
                                    az://.]
    --local-directory PATH          Directory on your local machine to which the
                                    files are downloaded.  [default: data]
  ```

Good luck! If you have any questions you can always visit the user forum:
  
  https://community.drivendata.org/c/cloud-cover